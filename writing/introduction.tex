\chapter{Introduction}
\section{Problem Statement}
Automatic text analysis and classification have become important issues with the rise of the internet and digital text.  Every minute, millions of emails and texts are sent, and at least thousands of news articles are published. \cite{jj17,mr16}.  With this huge rise in information, tools have been deployed to protect people from malicious actors. 

Spam emails are not only annoying but dangerous.  They often contain phishing scams which attempt to obtain identifying credentials from targets, or links to malicious software.  In 2002, a very effective email spam filter was developed which outperformed competitors by a large margin. \cite{bl02} The success of the algorithm was afforded by the implementation of Bayesian filtering.  The inventor of the algorithm, Paul Graham, applied basic rules of probability to the problem of classifying an email as spam or not.  Both spam filters and spam sources have of course evolved since then, both constantly attempting to overcome the other.

The term ``fake news'' which has recently come into existence.  Fake news refers to a news article which is intentionally created to misguide the readers with false statements.  With the new-found ease of creating a website that looks like a legitimate newspaper, it can be difficult to tell whether a news article is real or not.  The authors of \cite{nr17} suggest a machine learning solution, where an algorithm automatically separates fake news from true.

The huge rise of digital communication has also led to the expansion of forensic linguistics.  Automatic web scraping and search engine reporting can help investigative authorities identify criminals and terrorists on the internet.  Because of the huge amount of data to sift through, it is unlikely that the vast majority of text is analyzed by humans.  While organizations like the NSA, FBI, and, CIA are rightfully secretive about their methods, it is likely that they use some kind of pattern recognition or machine learning to identify suspicious individuals and organizations.

Recently, machine learning algorithms have been found vulnerable to adversarial attacks which cause them to misclassify samples after only minor alterations. \cite{cs14} While these adversarial attacks originally targeting image classifiers, they have been extended, to some extent, to the domain of language. \cite{np16} These attacks stand to compromise the methods of defense just previously described.  Neural networks in particular have been found extremely useful in those defenses, but also very vulnerable to adversarial attacks.  This thesis takes the perspective of the red team, and attempts to find efficient and effective methods of attack so that defenses may be correspondingly developed.

\section{Our Contribution}
This thesis contributes two algorithms for causing misclassification in recurrent neural networks acting on text.  The method found in \cite{np16} was an iterative method completely based on the gradient of the network with respect to the inputs.  This thesis finds the information provided by the gradient to unreliable, and describes search based methods instead.  One method, window search, does not even require a differentiable model.  The other method, gradient assisted window search, is a hybrid model which uses the gradient to search more quickly, though with degraded results.  

This work measures the performance of adversarial algorithms by counting the number of word replacements that are required to cause a misclassification.  Time to obtain a misclassification was also measured.  Both window search and gradient assisted window search show a very large improvement over the algorithm in \cite{np16} in a white box scenario, and an even larger improvement in a black box scenario.

\section{Overview}
Chapter 2 provides a background on machine learning, especially the algorithms employed in the experiments.  These algorithms include recurrently neural networks, word2vec, and gradient based training algorithms like gradient descent.  Chapter 3 describes the problem statement in more detail and develops the terminology and definitions to discuss it.  Chapter 4 briefly discusses the results obtained from training recurrent neural networks and a word embedding.  It also provides motivation for reducing the impact of the gradient on decision making in the algorithms.  Chapter 5 describes the developed algorithms in detail, and provides a time and space complexity analysis.  Chapter 6 gives a discussion of the results under several different scenarios.  Chapter 7 concludes the thesis with a summary of results and methods, as well as a discussion of future work.
