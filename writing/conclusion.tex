\chapter{Conclusion}
We implemented several new strategies for text based adversarial derivation, particularly targeting recurrent neural networks.  The strategies we employed offered significant improvement over previous methods in terms of average number of word substitutions.  The various optimizations we used offered a significant decrease in time spent per text sample, especially in larger cases, but ultimately degraded performance and caused failure for a significant amount of samples.  Our methods show some amount of transferability, still working across models with different hyperparameters, but at significantly degraded performance.  
\section{Future Work}
\begin{itemize}
\item Increased Complexity\\
These algorithms should be tested in producing similar results with more complex neural networks since we tested only single layer RNN's.  It is not clear whether more complex neural networks will be more or less susceptible to attacks of this type or attacks in general.  Some recomendations include adding more layers, making the layers larger, and changing the word embedding dimension.

\item Improving Semantic Similarity\\
In many cases words were replaced with nonsensical words which were either associated with very negative or very positive reviews.  While the number of words replaced might be small, it can be easily noticed, and a defense method might even work by detecting these common replacement words where they don't belong.

\item Testing on Larger Datasets\\
While other adversarial techniques have been shown to effective even for very large training sets, our training set is relatively small and therefore may be more vulnerable than something trained on more data.  In particular, the word embedding, while trained on 31 million words, still contains words which are quite rare in the list of the most frequenct 10,000.

\item Testing Across Different Word Embeddings\\
In our testing we assumed that the word embedding was held constant across all models.  This is not a totally unreasonable assumption since popular word embeddings are in fact published online and usually are not recreated for a new project.  That being said, a defense technique may attempt to either retrain a word embedding or alter it in some way to create a less vulnerable model.
\end{itemize}
