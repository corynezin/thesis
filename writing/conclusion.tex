\chapter{Conclusion}
The goal of our study was to generate attacks capable of changing a correct text classification of a recurrent neural network into an incorrect one with as few word substitutions as possible.  We implemented several new strategies for text based adversarial derivation, particularly targeting recurrent neural networks.  

White box, gray box, and black box scenarios were used to test two new algorithms against a baseline algorithm, fast gradient sign method.  The strategies employed offered significant improvement over previous methods in terms of average number of word substitutions.  The gradient based algorithm significantly increased the algorithm speed, though with slightly worse performance.  The new algorithms show some amount of transferability, still working across models with different hyperparameters, but at degraded performance.  

These algorithms should be tested in producing similar results with more complex neural networks since we tested only single layer RNN's.  It is not clear whether more complex neural networks will be more or less susceptible to attacks of this type or attacks in general.  Some recommendations include adding more layers, making the layers larger, and changing the word embedding dimension.

In many cases words were replaced with nonsensical words which were either associated with very negative or very positive reviews.  While the number of words replaced might be small, it can be easily noticed, and a defense method might even work by detecting these common replacement words where they don't belong.

While other adversarial techniques have been shown to effective even for very large training sets, our training set is relatively small and therefore may be more vulnerable than something trained on more data.  In particular, the word embedding, while trained on 31 million words, still contains words which are quite rare in the list of the most frequent 10,000.

In our testing we assumed that the word embedding was held constant across all models.  This is not a totally unreasonable assumption since popular word embeddings are in fact published online and usually are not recreated for a new project.  That being said, a defense technique may attempt to either retrain a word embedding or alter it in some way to create a less vulnerable model.

Ultimately the purpose of this work is to allow others to develop effective defenses.  Any of the previous suggestions may aid in developing a defense, but there are plenty of defenses for other attacks already in place which may be suitable in this case.  One simple defense might be achieved by running the attack on training samples and inserting the result into the training set.
