Consider the common scenario of a text classifier which maps plain text files to one of several classes.  It is common for the plain text to first be processed into a sequence of tokens, which are then each assigned an integer resulting in a sequence of integers.

\begin{definition}
Let $s$ be a sequence of characters.  Let $a_n \in \{0,1,\dots,V\} \forall n \in \{0,1,\dots,N\}$ and $E:s\rightarrow \{a_n\}_{n=1}^{N_s}$ then we call $E$ an encoder, $V$ the encoder vocabulary size, and $N_s$ the sample length with respect to $E$.
\end{definition}

In plain words, an encoder maps a string to a sequence of bounded integers.  The sequence is some length which depends on both the encoder and the string.  We assume a fixed encoder, and therefore vocabulary size, $V$.  Since, after encoding, the distance between one word and another is arbitrary, we further translate into a one-hot encoded vector.  That is, the integer $n$ is mapped to a vector where the $n^{th}$ element is $1$ and all others are $0$.  This ensures that all vectors representing words are unit norm and the distance between any two different words is the same.  We denote the set of one-hot encoded vectors of size $V$ as $1_V$

This simple method of representing words as vectors results in a very high dimension representation of all words in the vocabulary, and thus even a very simple linear model would be very large and be difficult train.  Using the word2vec model \cite{tm13}, the dimension of this representation can be significantly reduced, while also encoding information about statistical semantic similarity about each word.

\begin{definition}
Let $f: 1_V \rightarrow \mathbb{R}^D$.  We call $f$ a word embedding and we call $D$ the size of $f$, or embedding size.
\end{definition}

\noindent
Let $W \in M_{D\times V}(\mathbb{R})$.  Then clearly any word embedding, $f$, of size $D$ may be represented as the matrix multiplication $Wv$ $\forall v \in 1_V$.  The matrix $W$ is called the embedding matrix.

This numerical representation of words is extremely useful since we can now apply more general and modern techniques to solving the problem of classification.  We used gradient descent to train a word2vec model with noise contrastive estimation.  The hyperparameters we chose are shown in table \ref{tab:embedding_hperparameters}

\begin{table}[h]
\centering
\begin{tabular}{ l | r }
    \hline
    Learning rate & 1.0 \\
    Batch size & 128 \\
    Number of Batches & 100,000 \\
    Embedding size & 128 \\
    Number of skip windows & 8 \\
    Size of skip windows & 4 \\
    Vocabulary size & 10,000 \\
    \hline
\end{tabular}
\caption{Word embedding hyperparameters}
\label{tab:embedding_hperparameters}
\end{table}

Our corpus was the concatenation of all preprocessed training samples from the training set in the ``Large Movie Review Datasets.'' \cite{am11}  Preprocessing consisted of the steps laid out in table \ref{tab:preproc}

\begin{table}[h]
\centering
\begin{tabular}{ l | l }
    \hline
    Start & The movie isn't \verb|{<br \><br />}|good, I give it a 1\\
    Convert to lower case & the movie isn't \verb|{<br \><br />}|good, i give it a 1\\
    Remove HTML & the movie isn't \space good, i give it a 1\\
    Expand contractions & the movie is not \space good, i give it a 1\\
    Remove punctuation & the movie is not \space good i give it a 1\\
    Expand numbers & the movie is not \space good i give it a one\\
    Remove extra whitespace & the movie is not good i give it a one\\
    \hline
\end{tabular}
\caption{Preprocessing algorithm}
\label{tab:preproc}
\end{table}

The processing resulted in a corpus of 5,887,178 words totaling 31,985,233 characters.  Since there are 25,000 training samples, each review is on average about 234 words, and 1279 characters.  Of all 25,000 reviews training reviews, 27 had more than 1024 words.  The word embedding converged to an average noise contrastive loss of about 5.07.   Semantic difference between two words is measured by the angular distance between their embeddings, that is,

$$ \frac{cos^{-1}\left(\frac{v^Tu}{||v||_2||u||_2}\right)}{\pi}$$

\noindent
The eight nearest neighbors for a few common words are shown in table \ref{tab:nearest_words}.  We can see that the first few nearest neighbors are fairly high quality, and would usually make grammatical sense for replacement in a sentence.  The quality of replacement falls off quickly after that however.

\begin{table}[h]
\centering
\begin{tabular}{ | c |  c  c  c  c  c  c  c  c | }
    \hline
    all & but& some& and& UNK& just& also& that& so \\ \hline
    and & UNK& with& but& also& which& simpler& nerd& just \\ \hline
    will & can& would& if& do& could& to& did& you \\ \hline
    of & in& from& with& for& UNK& about& which& that \\ \hline
    her & his& she& him& he& their& UNK& the& india \\ \hline
    she & he& her& him& his& who& UNK& it& that \\ \hline
    most & all& best& films& which& other& UNK& some& only \\ \hline
    one & three& two& zero& five& only& nine& s& UNK \\ \hline
    movie & film& show& story& it& but& really& that& just \\ \hline 
    film & movie& story& show& which& UNK& it& that& but \\ \hline
\end{tabular}
\caption{Some examples of nearest neighbors in embedding space}
\label{tab:nearest_words}
\end{table}
