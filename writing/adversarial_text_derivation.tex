\noindent
Now that we have a clear idea of both the domain and numerical representation of words, we may define an adversarial derivation of textual data in the context of a classification model, $f$.  As per the definition of an adversarial derivation, we need only to define the model tolerance, $\epsilon$, as well as the domain metric, $d_D$ and codomain metric, $d_C$.  We will consider primarily two definitions.

\begin{definition}
Let $\{v_i\}_{i=1}^N$ be the sequence of vectors obtained from a given word embedding and text sample, then a discrete adversrial derivation is defined has having domain metric, $d_D(v,v^*) = \sum_i^N\rho(v_i,v_i^*)$, codomain metric $d_C(f(v),f(v^*)) = \rho(f(v),f(v^*))$, and tolerance $\epsilon = 1$.
\end{definition}

That is, a discrete adversarial derivation $\{v_i^*\}$ of sample $\{v_i\}$ is the sample which changes the least amount of words possible, while changing the classification.  This definition is simple, though it may not yield very good results if solved.  For example, a positive movie review, ``This movie was good'' could easily be changed to a negative review by changing just one word resulting in ``This movie was bad''.  These two samples would obviously have different sentiments if read by a human.

Clearly the codomain metric, $d_C$ and difference, $\epsilon$ make sense for any defintion in this context, but the domain metric has room for improvement which brings us to the next definition.

\begin{definition}
A semantic adversarial derivation is defined as having angular distance as the domain metric and the same difference and codomain metric as a discrete adversarial derivation.
\end{definition}

If we minimize this objective, then we would tend to use semantically similar words in substitution.  However, this does not necessarily solve the problem of actual sentiment inversion.  For instance, in our embedding the semantically closest (measured with the $l^2$ norm) word to ``bad'' is ``good.''  This makes sense since they are semantically very similar and would be used in the same contexts, but may result in obvious semantic flips.  We will therefore look to other metrics in an attempt to find better results.

Here we will focus on one particular type of model which has proven very effective in text classification and prediction, a recurrent neural network utilizing long short-term memory (LSTM) units.  Our base model has one layer of LSTM units where the output of each unit is averaged over time followed by a fully connected layer with two outputs, corresponding to the logits of a positive and negative class.  The network's confidence of a given samples class is given by the softmax of both logits.  

The model was trained for 50 epochs over the entire training set with a batch size of 1000 and a maximum unfolding length of 1024, meaning that 27 reviews would be clipped.  The Adam optimizer with exponential step decay factor of 0.8 every 500 batches was used to minimize the model loss, softmax cross entropy.  We used peepholes as well as output dropout for training.  

We trained fourteen models, varying the number of hidden units and initial learning rates.  The final training and testing accuracies are shown in tables \ref{tab:train_accuracy} and \ref{tab:test_accuracy} respectively.  As expected, increasing the model size always increased the training accuracy, as did increasing the learning rate.  The testing accuracy was less predictable.  Most accuracies were in the neighborhood of 80\%, 30\% greater than the baseline of 50\% random guessing.

\begin{table}
\centering
\begin{tabular}{ |c|c|c|c|c|c|c|}
    \hline
    \multicolumn{7}{|c|}{Number of Hidden Units, Learning Rate = 0.01}\\ \hline
    2 & 4 & 8 & 16 & 32 & 64 & 128 \\ \hline
    0.778 & 0.784 & 0.812 & 0.850 & 0.878 & 0.904 & 0.961 \\ \hline
    \multicolumn{7}{|c|}{Number of Hidden Units, Learning Rate = 0.1}\\ \hline
    2 & 4 & 8 & 16 & 32 & 64 & 128 \\ \hline
    0.827 & 0.853 & 0.895 & 0.965 & 0.975 & 0.997 & 0.990 \\ \hline
\end{tabular}
\caption{Training Accuracy}
\label{tab:train_accuracy}
\end{table}

\begin{table}
\centering
\begin{tabular}{ |c|c|c|c|c|c|c|}
    \hline
    \multicolumn{7}{|c|}{Number of Hidden Units, Learning Rate = 0.01}\\ \hline
    2 & 4 & 8 & 16 & 32 & 64 & 128 \\ \hline
    0.753 & 0.755 & 0.771 & 0.809 & 0.816 & 0.829 & 0.834 \\ \hline
    \multicolumn{7}{|c|}{Number of Hidden Units, Learning Rate = 0.1}\\ \hline
    2 & 4 & 8 & 16 & 32 & 64 & 128 \\ \hline
    0.793 & 0.816 & 0.827 & 0.822 & 0.815 & 0.809 & 0.817 \\ \hline
\end{tabular}
\caption{Testing Accuracy}
\label{tab:test_accuracy}
\end{table}

